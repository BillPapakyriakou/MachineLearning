{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyModVAAUJOLicKt0D6CZizu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BillPapakyriakou/MachineLearning/blob/main/MachineLearning1A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "byfSYcuHlvv9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "fashion_mnist = fetch_openml(name='Fashion-MNIST')\n",
        "\n",
        "# Split data into characteristics and labels\n",
        "X = fashion_mnist.data.astype('float32')\n",
        "y = fashion_mnist.target.astype('int64')\n",
        "\n",
        "# Split into train data and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Apply max pooling gradually to obtain 4x4 images\n",
        "X_train_pooled = X_train.reshape(-1, 28, 28)\n",
        "X_test_pooled = X_test.reshape(-1, 28, 28)\n",
        "\n",
        "# Apply max pooling from 28x28 to 14x14\n",
        "X_train_pooled = X_train_pooled.reshape(-1, 14, 2, 14, 2).max(axis=(2, 4))\n",
        "X_test_pooled = X_test_pooled.reshape(-1, 14, 2, 14, 2).max(axis=(2, 4))\n",
        "\n",
        "# Apply max pooling from 14x14 to 7x7\n",
        "X_train_pooled = X_train_pooled.reshape(-1, 7, 2, 7, 2).max(axis=(2, 4))\n",
        "X_test_pooled = X_test_pooled.reshape(-1, 7, 2, 7, 2).max(axis=(2, 4))\n",
        "\n",
        "# Apply max pooling from 7x7 to 4x4\n",
        "\"\"\" needs work done\"\"\"\n",
        "\n",
        "\"\"\"test dimensions\"\"\"\n",
        "#print(\"Shape of X_train_pooled:\", X_train_pooled.shape)\n",
        "#print(\"Shape of X_test_pooled:\", X_test_pooled.shape)\n",
        "\n",
        "# Convert data to vectors\n",
        "X_train_vectorized = X_train_pooled.reshape(len(X_train_pooled), -1)\n",
        "X_test_vectorized = X_test_pooled.reshape(len(X_test_pooled), -1)\n",
        "\n",
        "\"\"\"\n",
        "# Check the shape of X_train_vectorized\n",
        "print(\"Shape of X_train_vectorized:\", X_train_vectorized.shape)\n",
        "\n",
        "# Check the shape of X_test_vectorized\n",
        "print(\"Shape of X_test_vectorized:\", X_test_vectorized.shape)\n",
        "\"\"\"\n",
        "\n",
        "# Define model parameters\n",
        "K_VALUES = [1, 3, 5]\n",
        "C_VALUES = [1, 10, 100]\n",
        "RBF_VALUES = [0.02, 0.1, 1]\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "# Create models\n",
        "models = {}\n",
        "\n",
        "# ÎšNN Classifier\n",
        "for k in K_VALUES:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_vectorized, y_train)\n",
        "    models[f'KNN (K={k})'] = knn\n",
        "\n",
        "# Decision Tree Classifier\n",
        "dtc = DecisionTreeClassifier(max_depth=10)\n",
        "dtc.fit(X_train_vectorized, y_train)\n",
        "models['Decision Tree'] = dtc\n",
        "\n",
        "# Random Forest Classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "rfc.fit(X_train_vectorized, y_train)\n",
        "models['Random Forest'] = rfc\n",
        "\n",
        "# Linear SVM Classifier\n",
        "for c in C_VALUES:\n",
        "    svm_linear = SVC(kernel='linear', C=c, max_iter=500)\n",
        "    svm_linear.fit(X_train_vectorized, y_train)\n",
        "    models[f'Linear SVM (C={c})'] = svm_linear\n",
        "\n",
        "# SVM with RBF Kernel Classifier\n",
        "for gamma in RBF_VALUES:\n",
        "    svm_rbf = SVC(kernel='rbf', gamma=gamma, max_iter=500)\n",
        "    svm_rbf.fit(X_train_vectorized, y_train)\n",
        "    models[f'SVM RBF (gamma={gamma})'] = svm_rbf\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwwdYqQml6Qs",
        "outputId": "9fbd8447-dd86-4033-f13a-76d1a98557e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network (feed-forward)\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.fc1 = nn.Linear(49, 100)  # first hidden layer, 100 neurons\n",
        "    self.fc2 = nn.Linear(100, 100)  # second hidden layer, 100 neurons\n",
        "    self.fc3 = nn.Linear(100, 50)  # third hidden layer, 50 neurons\n",
        "    self.fc4 = nn.Linear(50, len(np.unique(y_train)))  # output layer\n",
        "\n",
        "    # initialize objects for the activation functions used in this model\n",
        "    self.leaky_relu = nn.LeakyReLU()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.leaky_relu(self.fc1(x))  # LeakyReLU for hidden layer 1\n",
        "    x = self.leaky_relu(self.fc2(x))  # LeakyReLU for hidden layer 2\n",
        "    x = self.leaky_relu(self.fc3(x))  # LeakyReLU for hidden layer 3\n",
        "    x = self.fc4(x)\n",
        "    x = self.softmax(x)  # SoftMax for output layer\n",
        "    return x\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_vectorized)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Create and train Neural Network\n",
        "model_nn = NeuralNetwork()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.NLLLoss()  #  Negative Log Likelihood loss function\n",
        "optimizer = optim.Adam(model_nn.parameters())  # Adam optimizer for training\n",
        "\n",
        "losses = []\n",
        "\n"
      ],
      "metadata": {
        "id": "5q5vKFB3N4nN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}